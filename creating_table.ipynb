{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab7b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b9a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s3_bucket='Buckets/planetx-ab-xfer-moodys-datasource'\n",
    "raw_path_dir='MoodysData/avs5'\n",
    "raw_path=f\"s3://{raw_s3_bucket}/{raw_path_dir}\"\n",
    "raw_df=wr.s3.read_csv(path=raw_path,path_suffix=['.csv'],dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba96391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc098500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shamim\n"
     ]
    }
   ],
   "source": [
    "print(\"shamim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "688f2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0fc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3=boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='AKIA2UNAHYF2ZKTJZA4Y',\n",
    "    aws_secret_access_key='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'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a271f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab-planetx-cms-dev\n",
      "ab-planetx-moodys-datasource\n",
      "ab-planetx-recommendation-engine-bucket\n",
      "ab-planetx-web\n",
      "adnandataplanetx\n",
      "mike-brand-test\n",
      "planetx-ab-xfer-datafinit-datasource\n",
      "planetx-ab-xfer-moodys-datasource\n",
      "planetx-airbyte-stage\n",
      "planetx-backend-dev\n",
      "planetx-badge-keywords\n",
      "planetx-badges\n",
      "planetx-brand-website\n",
      "planetx-datafiniti-datasource-ab\n",
      "planetx-finch-raw\n",
      "planetx-idaciti-raw-data\n",
      "planetx-incoming-data-sources\n",
      "planetx-johns-testing\n",
      "planetx-logical-data-models\n",
      "planetx-logos-storage\n",
      "planetx-misc-data-sources\n",
      "planetx-product-category-model-data\n",
      "planetx-refinitv-raw\n",
      "planetx-research-s3-users\n",
      "planetx-sandbox-acc-planetx-scoring-runs\n",
      "planetx-scratch-promptcloud-raw\n",
      "planetx-supply-chain\n",
      "scratch-planetx-external-poc\n",
      "scratch-planetx-legacy-sources\n",
      "scratch-planetx-moodys\n",
      "scratch-planetx-planetx-cms-dev\n",
      "scratch-planetx-raw-scores\n",
      "scratch-planetx-webs-io\n",
      "test-s35\n"
     ]
    }
   ],
   "source": [
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c937b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"first_30.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6e29393",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket('planetx-ab-xfer-moodys-datasource').upload_file(Filename='first_30.csv',Key='first_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8227f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=s3.Bucket('planetx-ab-xfer-moodys-datasource').Object('test.csv').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0970dc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6NDQBGDT9RPZMCKT',\n",
       "  'HostId': 'gjIXfWr9mD3v23hT8aJi9beYyt3QVCHObeRfNxouvYDTGGX709d8ZjY+WL1HHwM5s77hso4miw1thDqZbJmHUg==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'gjIXfWr9mD3v23hT8aJi9beYyt3QVCHObeRfNxouvYDTGGX709d8ZjY+WL1HHwM5s77hso4miw1thDqZbJmHUg==',\n",
       "   'x-amz-request-id': '6NDQBGDT9RPZMCKT',\n",
       "   'date': 'Fri, 05 May 2023 20:43:09 GMT',\n",
       "   'last-modified': 'Fri, 05 May 2023 20:42:48 GMT',\n",
       "   'etag': '\"a0ca1d18232b93a60791f7fee4562dc5\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '167'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2023, 5, 5, 20, 42, 48, tzinfo=tzutc()),\n",
       " 'ContentLength': 167,\n",
       " 'ETag': '\"a0ca1d18232b93a60791f7fee4562dc5\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {},\n",
       " 'Body': <botocore.response.StreamingBody at 0x23ceeddd8a0>}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8b1233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_csv=pd.read_csv(obj['Body'],index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa8340da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>49986723</th>\n",
       "      <th>20211202</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US710415188</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US911646860</th>\n",
       "      <td>1812590</td>\n",
       "      <td>20210504</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN9360002629</th>\n",
       "      <td>84770824</td>\n",
       "      <td>20210618</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US942404110</th>\n",
       "      <td>411117</td>\n",
       "      <td>20211203</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US942404110</th>\n",
       "      <td>411117</td>\n",
       "      <td>20201001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              49986723  20211202  new\n",
       "US710415188                          \n",
       "US911646860    1812590  20210504   10\n",
       "CN9360002629  84770824  20210618   10\n",
       "US942404110     411117  20211203   10\n",
       "US942404110     411117  20201001   10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readable_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "211f675c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'M9YSH7RM10J4QYZ9',\n",
       "  'HostId': 'AOB+iW1V4yJlkJx+9lwmCeO/jp3jPL2UNZxm3V9z6IrWsO+Lq6z948nWALOgPRq6Hpn+e8ZhGaGDosAEAyXSmw==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'AOB+iW1V4yJlkJx+9lwmCeO/jp3jPL2UNZxm3V9z6IrWsO+Lq6z948nWALOgPRq6Hpn+e8ZhGaGDosAEAyXSmw==',\n",
       "   'x-amz-request-id': 'M9YSH7RM10J4QYZ9',\n",
       "   'date': 'Fri, 05 May 2023 20:42:48 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"a0ca1d18232b93a60791f7fee4562dc5\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"a0ca1d18232b93a60791f7fee4562dc5\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define bucket and file information\n",
    "bucket_name = 'planetx-ab-xfer-moodys-datasource'\n",
    "file_name = 'test.csv'\n",
    "\n",
    "# read existing CSV file from S3 into a Pandas DataFrame\n",
    "obj = s3.Object(bucket_name, file_name)\n",
    "body = obj.get()['Body']\n",
    "csv_content = pd.read_csv(body)\n",
    "\n",
    "# modify the DataFrame as needed\n",
    "csv_content['new'] = '10'\n",
    "\n",
    "# write the modified DataFrame back to S3 as a CSV file\n",
    "csv_buffer = csv_content.to_csv(index=False)\n",
    "s3.Object(bucket_name, file_name).put(Body=csv_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40725568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': ['a', 'b', 'c']})\n",
    "\n",
    "# Convert the Pandas DataFrame to a PyArrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write the PyArrow Table to a Parquet file\n",
    "pq.write_table(table, 'example.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba73375",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m metadata \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mShamim Hasan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m32\u001b[39m}\n\u001b[0;32m     21\u001b[0m \u001b[39m# Create a PyArrow Parquet file writer with metadata_collector\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m writer \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mParquetWriter(\u001b[39m'\u001b[39;49m\u001b[39mexample.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m, table\u001b[39m.\u001b[39;49mschema, metadata\u001b[39m=\u001b[39;49mmetadata, metadata_collector\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPYARROW\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Write the PyArrow Table to the Parquet file\u001b[39;00m\n\u001b[0;32m     25\u001b[0m writer\u001b[39m.\u001b[39mwrite_table(table)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyarrow\\parquet\\core.py:990\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmetadata_collector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    989\u001b[0m engine_version \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mV2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m _parquet\u001b[39m.\u001b[39;49mParquetWriter(\n\u001b[0;32m    991\u001b[0m     sink, schema,\n\u001b[0;32m    992\u001b[0m     version\u001b[39m=\u001b[39;49mversion,\n\u001b[0;32m    993\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    994\u001b[0m     use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[0;32m    995\u001b[0m     write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[0;32m    996\u001b[0m     use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_deprecated_int96_timestamps,\n\u001b[0;32m    997\u001b[0m     compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[0;32m    998\u001b[0m     use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[0;32m    999\u001b[0m     column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[0;32m   1000\u001b[0m     writer_engine_version\u001b[39m=\u001b[39;49mengine_version,\n\u001b[0;32m   1001\u001b[0m     data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[0;32m   1002\u001b[0m     use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[0;32m   1003\u001b[0m     encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[0;32m   1004\u001b[0m     write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[0;32m   1005\u001b[0m     dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[0;32m   1006\u001b[0m     store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[0;32m   1007\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m   1008\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_open \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyarrow\\_parquet.pyx:1693\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'metadata'"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Define a schema for the Parquet table\n",
    "schema = pa.schema([\n",
    "    (\"name\", pa.string()),\n",
    "    (\"age\", pa.int64())\n",
    "])\n",
    "\n",
    "# Create a PyArrow Table from some data\n",
    "data = {\n",
    "    \"name\": pa.array([\"Alice\", \"Bob\", \"Charlie\"]),\n",
    "    \"age\": pa.array([25, 30, 35])\n",
    "}\n",
    "table = pa.Table.from_arrays([data[k] for k in schema.names], schema=schema)\n",
    "\n",
    "# Create a metadata collector and add metadata to it\n",
    "metadata = {'name': 'Shamim Hasan', 'age': 32}\n",
    "\n",
    "\n",
    "# Create a PyArrow Parquet file writer with metadata_collector\n",
    "writer = pq.ParquetWriter('example.parquet', table.schema, metadata=metadata, metadata_collector=\"PYARROW\")\n",
    "\n",
    "# Write the PyArrow Table to the Parquet file\n",
    "writer.write_table(table)\n",
    "\n",
    "# Close the file writer\n",
    "writer.close()\n",
    "\n",
    "# Read the Parquet file with metadata using ParquetDataset\n",
    "dataset = pq.ParquetDataset('example.parquet', metadata_collector=metadata_collector)\n",
    "\n",
    "# Print the schema and metadata of the Parquet table\n",
    "print(dataset.schema)\n",
    "print(dataset.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48254bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Create a DataFrame with complex data\n",
    "data = {'name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'age': [25, 30, 35],\n",
    "        'address': [{'street': '123 Main St', 'city': 'Anytown'},\n",
    "                    {'street': '456 Elm St', 'city': 'Otherville'},\n",
    "                    {'street': '789 Oak St', 'city': 'Smalltown'}]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to a PyArrow table\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write the table as a Parquet file\n",
    "pq.write_table(table, 'example.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274a2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  age                                         address\n",
      "0    Alice   25    {'city': 'Anytown', 'street': '123 Main St'}\n",
      "1      Bob   30  {'city': 'Otherville', 'street': '456 Elm St'}\n",
      "2  Charlie   35   {'city': 'Smalltown', 'street': '789 Oak St'}\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read Parquet file\n",
    "table = pq.read_table('example.parquet')\n",
    "\n",
    "# Convert table to Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876bf19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m      4\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[0;32m      5\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, LongType()),\n\u001b[0;32m      6\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, StringType()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     ]))\n\u001b[0;32m     36\u001b[0m ])\n\u001b[0;32m     38\u001b[0m \u001b[39m# Create the table\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mCREATE TABLE struct_demo (id BIGINT, name STRING, employee_info STRUCT < employer: STRING, id: BIGINT, address: STRING >, places_lived ARRAY < STRUCT <street: STRING, city: STRING, country: STRING >>, memorable_moments MAP < STRING, STRUCT < year: INT, place: STRING, details: STRING >>, current_address STRUCT < street_address: STRUCT <street_number: INT, street_name: STRING, street_type: STRING>, country: STRING, postal_code: STRING >) STORED AS PARQUET\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39m# Alternatively, you can create the table using the defined schema\u001b[39;00m\n\u001b[0;32m     42\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mCREATE TABLE struct_demo USING PARQUET OPTIONS (path \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath/to/parquet/file\u001b[39m\u001b[39m'\u001b[39m\u001b[39m) AS SELECT * FROM VALUES (1, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mJohn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mABC Company\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 123, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m123 Main St.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m), array(named_struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m123 Main St.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mLos Angeles\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mUSA\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)), map(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mevent1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, named_struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 2022, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mplace\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSan Francisco\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mLorem ipsum\u001b[39m\u001b[39m'\u001b[39m\u001b[39m), \u001b[39m\u001b[39m'\u001b[39m\u001b[39mevent2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, named_struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 2021, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mplace\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mNew York\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mDolor sit amet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)), named_struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreet_address\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, named_struct(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreet_number\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 123, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreet_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mMain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstreet_type\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSt.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m), \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mUSA\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpostal_code\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m12345\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)) AS t(id, name, employee_info, places_lived, memorable_moments, current_address)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType, IntegerType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"employee_info\", StructType([\n",
    "        StructField(\"employer\", StringType()),\n",
    "        StructField(\"id\", LongType()),\n",
    "        StructField(\"address\", StringType())\n",
    "    ])),\n",
    "    StructField(\"places_lived\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"street\", StringType()),\n",
    "            StructField(\"city\", StringType()),\n",
    "            StructField(\"country\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"memorable_moments\", MapType(\n",
    "        StringType(),\n",
    "        StructType([\n",
    "            StructField(\"year\", IntegerType()),\n",
    "            StructField(\"place\", StringType()),\n",
    "            StructField(\"details\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"current_address\", StructType([\n",
    "        StructField(\"street_address\", StructType([\n",
    "            StructField(\"street_number\", IntegerType()),\n",
    "            StructField(\"street_name\", StringType()),\n",
    "            StructField(\"street_type\", StringType())\n",
    "        ])),\n",
    "        StructField(\"country\", StringType()),\n",
    "        StructField(\"postal_code\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Create the table\n",
    "spark.sql(\"CREATE TABLE struct_demo (id BIGINT, name STRING, employee_info STRUCT < employer: STRING, id: BIGINT, address: STRING >, places_lived ARRAY < STRUCT <street: STRING, city: STRING, country: STRING >>, memorable_moments MAP < STRING, STRUCT < year: INT, place: STRING, details: STRING >>, current_address STRUCT < street_address: STRUCT <street_number: INT, street_name: STRING, street_type: STRING>, country: STRING, postal_code: STRING >) STORED AS PARQUET\")\n",
    "\n",
    "# Alternatively, you can create the table using the defined schema\n",
    "spark.sql(\"CREATE TABLE struct_demo USING PARQUET OPTIONS (path 'path/to/parquet/file') AS SELECT * FROM VALUES (1, 'John', struct('ABC Company', 123, '123 Main St.'), array(named_struct('street', '123 Main St.', 'city', 'Los Angeles', 'country', 'USA')), map('event1', named_struct('year', 2022, 'place', 'San Francisco', 'details', 'Lorem ipsum'), 'event2', named_struct('year', 2021, 'place', 'New York', 'details', 'Dolor sit amet')), named_struct('street_address', named_struct('street_number', 123, 'street_name', 'Main', 'street_type', 'St.'), 'country', 'USA', 'postal_code', '12345')) AS t(id, name, employee_info, places_lived, memorable_moments, current_address)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac4d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb0a8bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m conf\u001b[39m=\u001b[39mSparkConf()\\\n\u001b[0;32m      2\u001b[0m     \u001b[39m.\u001b[39msetAppName(\u001b[39m'\u001b[39m\u001b[39mapp\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39msetMaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc\u001b[39m=\u001b[39mSparkContext(conf\u001b[39m=\u001b[39;49mconf)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 103\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conf=SparkConf()\\\n",
    "    .setAppName('app')\\\n",
    "    .setMaster(\"local[*]\")\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd2c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6dbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the data as a Pandas DataFrame\n",
    "data = {\n",
    "    'id': [1, 2],\n",
    "    'name': ['John', 'Jane'],\n",
    "    'employee_info': [\n",
    "        {'employer': 'ABC Company', 'id': 123, 'address': '123 Main St.'},\n",
    "        {'employer': 'XYZ Inc.', 'id': 456, 'address': '456 Elm St.'}\n",
    "    ],\n",
    "    'places_lived': [\n",
    "        [{'street': '123 Main St.', 'city': 'Los Angeles', 'country': 'USA'}],\n",
    "        [{'street': '456 Elm St.', 'city': 'New York', 'country': 'USA'},\n",
    "         {'street': '789 Oak St.', 'city': 'San Francisco', 'country': 'USA'}]\n",
    "    ],\n",
    "    'memorable_moments': [\n",
    "        {'event1': {'year': 2022, 'place': 'San Francisco', 'details': 'Lorem ipsum'}},\n",
    "        {'event1': {'year': 2021, 'place': 'New York', 'details': 'Dolor sit amet'},\n",
    "         'event2': {'year': 2023, 'place': 'London', 'details': 'Consectetur adipiscing elit'}}\n",
    "    ],\n",
    "    'current_address': [\n",
    "        {'street_address': {'street_number': 123, 'street_name': 'Main', 'street_type': 'St.'},\n",
    "         'country': 'USA', 'postal_code': '12345'},\n",
    "        {'street_address': {'street_number': 456, 'street_name': 'Elm', 'street_type': 'St.'},\n",
    "         'country': 'USA', 'postal_code': '45678'}\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the schema using PyArrow\n",
    "schema = pa.schema([\n",
    "    pa.field('id', pa.int64()),\n",
    "    pa.field('name', pa.string()),\n",
    "    pa.field('employee_info', pa.struct([\n",
    "        pa.field('employer', pa.string()),\n",
    "        pa.field('id', pa.int64()),\n",
    "        pa.field('address', pa.string())\n",
    "    ])),\n",
    "    pa.field('places_lived', pa.list_(pa.struct([\n",
    "        pa.field('street', pa.string()),\n",
    "        pa.field('city', pa.string()),\n",
    "        pa.field('country', pa.string())\n",
    "    ]))),\n",
    "    pa.field('memorable_moments', pa.map_(\n",
    "        pa.string(),\n",
    "        pa.struct([\n",
    "            pa.field('year', pa.int32()),\n",
    "            pa.field('place', pa.string()),\n",
    "            pa.field('details', pa.string())\n",
    "        ])\n",
    "    )),\n",
    "    pa.field('current_address', pa.struct([\n",
    "        pa.field('street_address', pa.struct([\n",
    "            pa.field('street_number', pa.int32()),\n",
    "            pa.field('street_name', pa.string()),\n",
    "            pa.field('street_type', pa.string())\n",
    "        ])),\n",
    "        pa.field('country', pa.string()),\n",
    "        pa.field('postal_code', pa.string())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Convert the Pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df, schema=schema)\n",
    "\n",
    "# Write the PyArrow Table to Parquet file\n",
    "# with pq.ParquetWriter(r'C:\\Users\\User\\PycharmProjects\\pythonProject3\\Planet_X\\Phase_1', schema) as writer:\n",
    "#     writer.write_table(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7604f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, r\"C:\\Users\\User\\PycharmProjects\\pythonProject3\\Planet_X\\Phase_1\\exam.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8f861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
